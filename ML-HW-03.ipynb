{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Homework 03\n",
    "\n",
    "## Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    weights: ndarray\n",
    "    X: ndarray\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / (np.sum(np.exp(x)) + 0.001) # account for divide by zero error\n",
    "    \n",
    "def predict(s):\n",
    "    if s >= 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def loss_function(weights, x, y, lr):\n",
    "    num_samples = x.shape[0]\n",
    "    s = softmax(np.matmul(x, weights.T))\n",
    "    #print(y)\n",
    "    #y = OneHotEncoder().fit_transform(y)\n",
    "    #y = np.array([1 if i == y else 0 for i in range(10)]).reshape(1, -1)\n",
    "    #print(y)\n",
    "    #print(y.shape, weights.T.shape, x.shape)\n",
    "    err = np.log(1 + np.exp(np.matmul(y, np.matmul(weights, x.T))))\n",
    "    return err\n",
    "    #return np.log(1 + np.exp(-y_predict * y_actual))\n",
    "    \n",
    "\n",
    "def loss_gradient(weights, x, y, lr):\n",
    "    num_samples = x.shape[0]\n",
    "    s = softmax(np.matmul(x, weights.T))\n",
    "    y = OneHotEncoder().fit_transform(y)\n",
    "    return (1 / num_samples) * (y - s)\n",
    "    \n",
    "\n",
    "def train_(x_train, y_train, lr, num_iters):\n",
    "    weights = np.ones((10, 20))\n",
    "    running_error_rate = []\n",
    "    x_len = len(x_train)\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        #print(weights)\n",
    "        accuracies = []\n",
    "        total_error = np.zeros(weights.shape)\n",
    "        for j in range(len(x_train)):\n",
    "\n",
    "            x_train_array = np.array(x_train[j]).reshape(1, -1)\n",
    "            ###print(x_train_array.shape)\n",
    "            z = np.matmul(weights, x_train[j])\n",
    "            s = softmax(z)\n",
    "            #print(s)\n",
    "            y_hat = np.argmax(s)\n",
    "            y_hat_array = np.array([1 if i == y_hat else 0 for i in range(10)]).reshape(-1, 1)\n",
    "            \n",
    "            y_actual = np.array([1 if i == y_train[i] else 0 for i in range(10)])\n",
    "            ###print(y_actual.reshape(1, -1).shape)\n",
    "            ###print(y_hat_array.shape)\n",
    "            #print(y_actual.reshape(1, -1).shape)\n",
    "            if y_hat == y_train[i]:\n",
    "                accuracies.append(1)\n",
    "            else:\n",
    "                accuracies.append(0)\n",
    "                total_error += np.abs(np.matmul(y_actual.reshape(-1, 1) - y_hat_array, x_train_array))\n",
    "        #print(total_error)\n",
    "        weights -= lr * total_error / x_len\n",
    "        running_error_rate.append(1 - np.sum(accuracies) / len(accuracies))\n",
    "        #print(accuracies)\n",
    "        \"\"\"\n",
    "        for i, row in enumerate(s):\n",
    "            #print(row.shape)\n",
    "            yhat = np.argmax(row)\n",
    "            y_actual = np.array([1 if i == y_train[i] else 0 for i in range(len(row))])\n",
    "            #print(y_actual.shape)\n",
    "            \n",
    "            if yhat == y_train[i]:\n",
    "                accuracies.append(1)\n",
    "            else:\n",
    "                accuracies.append(0)\n",
    "                weights[y_actual] -= np.transpose(np.array([lr * loss(row, y_actual)]))\n",
    "        running_error_rate.append(1 - np.sum(accuracies) / len(accuracies))\n",
    "        \"\"\"\n",
    "        \n",
    "    return running_error_rate\n",
    "\n",
    "\n",
    "\n",
    "def train(x_train, y_train, lr, num_iters):\n",
    "    weights = np.zeros((10, 20))\n",
    "    losses = []\n",
    "    \n",
    "    #print(weights.shape)\n",
    "    #print(x_train.shape)\n",
    "    for i in range(num_iters):\n",
    "        for j in range(len(x_train)):\n",
    "            x_arr = np.array(x_train[j]).reshape(1, -1)\n",
    "            y_arr = np.array([1 if i == y_train[j] else 0 for i in range(10)]).reshape(1, -1)\n",
    "            #y_arr = np.array(y_train[j]).reshape(1, -1)\n",
    "            #print(x_arr.shape, y_arr.shape)\n",
    "            loss = loss_function(weights, x_arr, y_arr, lr)\n",
    "            gradient = loss_gradient(weights, x_arr, y_arr, lr)\n",
    "            #loss = loss_function(weights, x_train[j], y_train[j], lr)\n",
    "            #gradient = loss_gradient(weights, x_train[j], y_train[j], lr)\n",
    "            \n",
    "            losses.append(loss)\n",
    "            weights -=  lr * gradient.T\n",
    "    #print(losses)\n",
    "    return weights, losses\n",
    "    \n",
    "def predict(x_test, y_test, weights):\n",
    "    yes = 0\n",
    "    for i, x in enumerate(x_test):\n",
    "        y_prob = softmax(np.matmul(weights, np.array(x).reshape(-1, 1)))\n",
    "        y_hat = np.argmax(y_prob)\n",
    "        if y_hat == y_test[i]:\n",
    "            yes += 1\n",
    "    return yes / len(y_test)\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing features\n",
    "def compute_x_symmetry(img):\n",
    "    # transform cell (i, j) |--> (i, #cols - j)\n",
    "    img_reflected = np.fliplr(img)\n",
    "    # compute symmetry as magnitude of vector of element-wise difference between matrices\n",
    "    # normalized by max possible pixel intensity\n",
    "    x_symmetry = np.subtract(img, img_reflected) / 16\n",
    "    x_symmetry = np.linalg.norm(x_symmetry.flatten())\n",
    "    return x_symmetry\n",
    "\n",
    "\n",
    "def compute_y_symmetry(img):\n",
    "    # transform cell (i, j) |--> (#rows - i, j)\n",
    "    img_reflected = np.flipud(img)\n",
    "    # compute symmetry as magnitude of vector of element-wise difference between matrices\n",
    "    # normalized by max possible pixel intensity\n",
    "    y_symmetry = np.subtract(img, img_reflected) / 16\n",
    "    y_symmetry = np.linalg.norm(y_symmetry.flatten())\n",
    "    return y_symmetry\n",
    "\n",
    "def compute_density(img):\n",
    "    # density is calculated simply the average of all pixel intensities\n",
    "    density = np.sum(img.flatten()) / 64\n",
    "    return density\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with digits dataset – sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n",
      "(1797, 64)\n"
     ]
    }
   ],
   "source": [
    "data = load_digits()\n",
    "print(data.data)\n",
    "print(data.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 ... 8 9 8]\n",
      "(1797,)\n"
     ]
    }
   ],
   "source": [
    "# let's see some of the labels from the list of 1797 labels\n",
    "print(data.target)\n",
    "print(data.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7faf074f8610>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's see what one of the images looks like\n",
    "img1 = data.images[0] # this should be a '0'\n",
    "plt.imshow(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(X_train=data):\n",
    "\n",
    "    # doing feature engineering – compute x symmetries, y symmetries, and density for each image\n",
    "    x_symmetries = {x: 0 for x in range(0, 10)}\n",
    "    y_symmetries = {y: 0 for y in range(0, 10)}\n",
    "    densities = {z: 0 for z in range(0, 10)}\n",
    "\n",
    "    # we will create a list of triples of features (represented as a list (of length 1797) of lists (of length 3))\n",
    "    feature_list = []\n",
    "    \n",
    "    for i, img in enumerate(X_train.images):\n",
    "        rx = compute_x_symmetry(img)\n",
    "        x_symmetries[X_train.target[i]] += rx\n",
    "    \n",
    "        ry = compute_y_symmetry(img)\n",
    "        y_symmetries[X_train.target[i]] += ry\n",
    "    \n",
    "        density = compute_density(img)\n",
    "        densities[X_train.target[i]] += density\n",
    "    \n",
    "        feature_list.append([rx, ry, density])\n",
    "    \n",
    "    feature_array = np.array(feature_list)\n",
    "        \n",
    "    targets_count = Counter(X_train.target)\n",
    "    \n",
    "    for i in range(10):\n",
    "        x_symmetries[i] /= targets_count[i]\n",
    "        y_symmetries[i] /= targets_count[i]\n",
    "        densities[i] /= targets_count[i]\n",
    "        \n",
    "    # we would like to transform this into a 20-dimensional feature space. do this via cubic transform\n",
    "    cubic = PolynomialFeatures(3)\n",
    "    transformed_feature_array = cubic.fit_transform(feature_array)/100\n",
    "    \n",
    "    avg_features = list(zip(x_symmetries.values(), y_symmetries.values(), densities.values()))\n",
    "    avg_features = np.array([np.array([x, y, z]) for (x,y,z) in avg_features])\n",
    "    transformed_avg_features = cubic.fit_transform(avg_features)\n",
    "    \n",
    "    return transformed_avg_features, transformed_feature_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "taf, tfa = feature_engineering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 20)\n",
      "(10, 20)\n",
      "[[0.01       0.0143069  0.01198958 ... 0.06603516 0.25301056 0.96939789]\n",
      " [0.01       0.02052057 0.01583607 ... 0.12264771 0.3787706  1.1697501 ]\n",
      " [0.01       0.03511143 0.02616056 ... 0.36785156 0.75579498 1.55287109]\n",
      " ...\n",
      " [0.01       0.01571226 0.01672386 ... 0.16344238 0.57111003 1.99560638]\n",
      " [0.01       0.02270738 0.02888555 ... 0.44847656 0.83452159 1.55287109]\n",
      " [0.01       0.02007797 0.02368412 ... 0.34357422 0.88852453 2.29783203]]\n"
     ]
    }
   ],
   "source": [
    "print(tfa.shape) # 1797 x 20, 20-dimensional feature space\n",
    "print(taf.shape) # 10 x 20\n",
    "print(tfa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.0001, 0.001, 0.01, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 6 8 ... 8 7 5]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfa, data.target, test_size=0.4)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09735744089012517\n"
     ]
    }
   ],
   "source": [
    "weights, losses = train(X_train, y_train, lr=0.01, num_iters=200)\n",
    "\n",
    "res = predict(X_test, y_test, weights=weights)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
